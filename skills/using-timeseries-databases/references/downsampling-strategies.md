# Downsampling Strategies for Time-Series Visualization

Reduce millions of data points to hundreds for smooth chart rendering while preserving visual fidelity.


## Table of Contents

- [The Problem](#the-problem)
- [LTTB Algorithm](#lttb-algorithm)
  - [How LTTB Works](#how-lttb-works)
  - [Visual Comparison](#visual-comparison)
- [Implementation](#implementation)
  - [TimescaleDB (Native)](#timescaledb-native)
  - [PostgreSQL (Custom Function)](#postgresql-custom-function)
  - [Python (Server-Side)](#python-server-side)
  - [TypeScript (Server-Side)](#typescript-server-side)
- [Alternative Strategies](#alternative-strategies)
  - [1. Average Per Bucket](#1-average-per-bucket)
  - [2. Min-Max Per Bucket](#2-min-max-per-bucket)
  - [3. Reservoir Sampling (Random)](#3-reservoir-sampling-random)
- [Adaptive Downsampling](#adaptive-downsampling)
- [API Design Patterns](#api-design-patterns)
  - [Pattern 1: Client Specifies Points](#pattern-1-client-specifies-points)
  - [Pattern 2: Automatic Downsampling](#pattern-2-automatic-downsampling)
- [Performance Benchmarks](#performance-benchmarks)
  - [LTTB Performance](#lttb-performance)
  - [Memory Usage](#memory-usage)
- [Best Practices](#best-practices)
- [Example: Complete FastAPI Integration](#example-complete-fastapi-integration)
- [Resources](#resources)

## The Problem

Web browsers struggle to render large datasets:
- 1M points in a LineChart: 2-5 second render time, sluggish interactions
- Network overhead: 10MB JSON response vs. 100KB downsampled
- User perception: Chart looks the same with 1,000 points vs. 1M points

**Solution**: Downsample server-side before sending to frontend.

## LTTB Algorithm

**Largest-Triangle-Three-Buckets** is the gold standard for time-series downsampling.

### How LTTB Works

1. Divide time range into N buckets (N = target point count)
2. For each bucket, select the point that forms the largest triangle with neighboring buckets
3. This preserves peaks, valleys, and overall shape

### Visual Comparison

```
Original (1000 points):          LTTB (100 points):
   ╱╲  ╱╲  ╱╲                       ╱╲    ╱╲
  ╱  ╲╱  ╲╱  ╲                     ╱  ╲  ╱  ╲
─╯           ╰─                  ─╯    ╲╱    ╰─

Random sampling (100 points):    Average per bucket (100 points):
   ╲     ╲                           ─────
    ╲  ╲                                ╲
─╯   ╲  ╰─  (misses peaks)       ─╯      ─  (smooths too much)
```

LTTB preserves visual features better than random sampling or averaging.

## Implementation

### TimescaleDB (Native)

TimescaleDB Toolkit provides built-in LTTB function:

```sql
-- Install toolkit
CREATE EXTENSION timescaledb_toolkit;

-- Downsample to 1000 points
SELECT time, value
FROM lttb(
  'SELECT time, temperature FROM sensor_data WHERE sensor_id = 123',
  1000  -- target number of points
);
```

### PostgreSQL (Custom Function)

```sql
CREATE OR REPLACE FUNCTION lttb_downsample(
  query TEXT,
  threshold INTEGER
) RETURNS TABLE(time TIMESTAMPTZ, value DOUBLE PRECISION) AS $$
DECLARE
  data_row RECORD;
  bucket_size INTEGER;
  every_bucket_point INTEGER;
  sampled_data RECORD[];
  i INTEGER := 0;
  max_area DOUBLE PRECISION;
  area DOUBLE PRECISION;
  next_point_index INTEGER;
  point_index INTEGER;
BEGIN
  -- Execute query and store in array
  FOR data_row IN EXECUTE query LOOP
    sampled_data[i] := data_row;
    i := i + 1;
  END LOOP;

  -- Calculate bucket size
  bucket_size := (array_length(sampled_data, 1) - 2) / (threshold - 2);

  -- First and last points always included
  RETURN QUERY SELECT sampled_data[0].time, sampled_data[0].value;

  -- Bucket selection logic (simplified for readability)
  -- Full implementation: https://github.com/sveinn-steinarsson/flot-downsample

  RETURN QUERY SELECT sampled_data[array_length(sampled_data, 1)].time,
                      sampled_data[array_length(sampled_data, 1)].value;
END;
$$ LANGUAGE plpgsql;
```

### Python (Server-Side)

```python
import numpy as np
from typing import List, Tuple

def lttb(data: List[Tuple[float, float]], threshold: int) -> List[Tuple[float, float]]:
    """
    Largest-Triangle-Three-Buckets downsampling algorithm.

    Args:
        data: List of (timestamp, value) tuples
        threshold: Target number of points

    Returns:
        Downsampled list of (timestamp, value) tuples
    """
    if len(data) <= threshold:
        return data

    # Convert to numpy for faster computation
    data_array = np.array(data)

    # Always include first and last points
    sampled = [data[0]]

    # Bucket size
    every = (len(data) - 2) / (threshold - 2)

    a = 0  # Initially a is the first point in the triangle

    for i in range(threshold - 2):
        # Calculate point average for next bucket (for area calculation)
        avg_x = 0
        avg_y = 0
        avg_range_start = int((i + 1) * every) + 1
        avg_range_end = int((i + 2) * every) + 1
        avg_range_end = min(avg_range_end, len(data))

        avg_range_length = avg_range_end - avg_range_start

        for j in range(avg_range_start, avg_range_end):
            avg_x += data_array[j, 0]
            avg_y += data_array[j, 1]

        avg_x /= avg_range_length
        avg_y /= avg_range_length

        # Get the range for this bucket
        range_offs = int(i * every) + 1
        range_to = int((i + 1) * every) + 1

        # Point A (previous selected point)
        point_a_x = data_array[a, 0]
        point_a_y = data_array[a, 1]

        max_area = -1

        for j in range(range_offs, range_to):
            # Calculate triangle area over three points
            area = abs(
                (point_a_x - avg_x) * (data_array[j, 1] - point_a_y) -
                (point_a_x - data_array[j, 0]) * (avg_y - point_a_y)
            ) * 0.5

            if area > max_area:
                max_area = area
                next_a = j  # Next a is this b

        sampled.append(tuple(data_array[next_a]))
        a = next_a  # This a is the next a (chosen b)

    # Always include last point
    sampled.append(data[-1])

    return sampled

# Usage
from datetime import datetime, timedelta

# Generate sample data
now = datetime.now()
data = [(now + timedelta(seconds=i), np.sin(i / 100) + np.random.random() * 0.1)
        for i in range(100000)]

# Downsample to 1000 points
downsampled = lttb(data, 1000)
print(f"Original: {len(data)} points, Downsampled: {len(downsampled)} points")
```

### TypeScript (Server-Side)

```typescript
interface DataPoint {
  time: Date;
  value: number;
}

function lttb(data: DataPoint[], threshold: number): DataPoint[] {
  if (data.length <= threshold) {
    return data;
  }

  const sampled: DataPoint[] = [data[0]];  // Always include first point

  const bucketSize = (data.length - 2) / (threshold - 2);

  let a = 0;  // Initially a is the first point

  for (let i = 0; i < threshold - 2; i++) {
    // Calculate point average for next bucket
    const avgRangeStart = Math.floor((i + 1) * bucketSize) + 1;
    const avgRangeEnd = Math.min(Math.floor((i + 2) * bucketSize) + 1, data.length);

    let avgX = 0;
    let avgY = 0;
    const avgRangeLength = avgRangeEnd - avgRangeStart;

    for (let j = avgRangeStart; j < avgRangeEnd; j++) {
      avgX += data[j].time.getTime();
      avgY += data[j].value;
    }
    avgX /= avgRangeLength;
    avgY /= avgRangeLength;

    // Get range for this bucket
    const rangeOffs = Math.floor(i * bucketSize) + 1;
    const rangeTo = Math.floor((i + 1) * bucketSize) + 1;

    const pointAX = data[a].time.getTime();
    const pointAY = data[a].value;

    let maxArea = -1;
    let nextA = 0;

    for (let j = rangeOffs; j < rangeTo; j++) {
      // Calculate triangle area
      const area = Math.abs(
        (pointAX - avgX) * (data[j].value - pointAY) -
        (pointAX - data[j].time.getTime()) * (avgY - pointAY)
      ) * 0.5;

      if (area > maxArea) {
        maxArea = area;
        nextA = j;
      }
    }

    sampled.push(data[nextA]);
    a = nextA;
  }

  sampled.push(data[data.length - 1]);  // Always include last point

  return sampled;
}

// Usage
const data: DataPoint[] = Array.from({ length: 100000 }, (_, i) => ({
  time: new Date(Date.now() + i * 1000),
  value: Math.sin(i / 100) + Math.random() * 0.1
}));

const downsampled = lttb(data, 1000);
console.log(`Original: ${data.length} points, Downsampled: ${downsampled.length} points`);
```

## Alternative Strategies

### 1. Average Per Bucket

Simplest approach: divide into N buckets, average each bucket.

```python
def avg_downsample(data: List[Tuple[float, float]], threshold: int) -> List[Tuple[float, float]]:
    if len(data) <= threshold:
        return data

    bucket_size = len(data) // threshold
    downsampled = []

    for i in range(0, len(data), bucket_size):
        bucket = data[i:i + bucket_size]
        avg_time = sum(p[0] for p in bucket) / len(bucket)
        avg_value = sum(p[1] for p in bucket) / len(bucket)
        downsampled.append((avg_time, avg_value))

    return downsampled
```

**Pros**: Simple, fast
**Cons**: Smooths peaks/valleys, loses visual detail

### 2. Min-Max Per Bucket

Keep min and max values per bucket (good for candlestick charts).

```python
def minmax_downsample(data: List[Tuple[float, float]], threshold: int) -> List[Tuple[float, float]]:
    if len(data) <= threshold:
        return data

    bucket_size = len(data) // (threshold // 2)  # Each bucket produces 2 points
    downsampled = []

    for i in range(0, len(data), bucket_size):
        bucket = data[i:i + bucket_size]
        values = [p[1] for p in bucket]
        min_point = min(bucket, key=lambda p: p[1])
        max_point = max(bucket, key=lambda p: p[1])

        # Add in time order
        if min_point[0] < max_point[0]:
            downsampled.extend([min_point, max_point])
        else:
            downsampled.extend([max_point, min_point])

    return downsampled
```

**Pros**: Preserves peaks and valleys
**Cons**: Double the points (threshold/2 buckets), can create zigzag artifacts

### 3. Reservoir Sampling (Random)

Randomly sample N points with uniform probability.

```python
import random

def reservoir_sample(data: List[Tuple[float, float]], threshold: int) -> List[Tuple[float, float]]:
    if len(data) <= threshold:
        return data

    reservoir = data[:threshold]

    for i in range(threshold, len(data)):
        j = random.randint(0, i)
        if j < threshold:
            reservoir[j] = data[i]

    return sorted(reservoir, key=lambda p: p[0])  # Sort by time
```

**Pros**: Statistically unbiased
**Cons**: Misses important peaks/valleys, poor visual fidelity

## Adaptive Downsampling

Adjust target points based on time range.

```python
def adaptive_threshold(time_range_seconds: int, chart_width_pixels: int = 800) -> int:
    """
    Calculate optimal number of points based on time range and chart width.

    Args:
        time_range_seconds: Time range in seconds
        chart_width_pixels: Chart width in pixels

    Returns:
        Optimal number of points
    """
    # 1 point per pixel is maximum useful resolution
    max_points = chart_width_pixels

    # Longer time ranges need fewer points
    if time_range_seconds < 3600:  # < 1 hour
        return min(1000, max_points)
    elif time_range_seconds < 86400:  # < 1 day
        return min(800, max_points)
    elif time_range_seconds < 604800:  # < 1 week
        return min(600, max_points)
    else:  # > 1 week
        return min(400, max_points)

# Usage
time_range = 7 * 24 * 3600  # 7 days in seconds
threshold = adaptive_threshold(time_range)
print(f"Recommended threshold: {threshold} points")
```

## API Design Patterns

### Pattern 1: Client Specifies Points

```typescript
// Frontend
const chartWidth = 800;
const response = await fetch(
  `/api/metrics?start=7d&metric=cpu_usage&points=${chartWidth}`
);
const data = await response.json();

// Backend (Python)
@app.get("/api/metrics")
async def get_metrics(start: str, metric: str, points: int = 800):
    # Parse start to get time range
    time_range_seconds = parse_duration(start)

    # Query raw data
    query = f"SELECT time, {metric} FROM metrics WHERE time > NOW() - INTERVAL '{start}'"
    data = execute_query(query)

    # Downsample if needed
    if len(data) > points:
        data = lttb(data, points)

    return data
```

### Pattern 2: Automatic Downsampling

```python
@app.get("/api/metrics")
async def get_metrics(start: str, metric: str):
    time_range_seconds = parse_duration(start)

    # Automatically choose data source based on time range
    if time_range_seconds < 3600:  # < 1 hour
        # Query raw data (no downsampling needed)
        query = "SELECT time, cpu FROM metrics WHERE time > NOW() - INTERVAL '1h'"
    elif time_range_seconds < 86400:  # < 1 day
        # Query 1-minute rollup
        query = "SELECT bucket, avg_cpu FROM metrics_1min WHERE bucket > NOW() - INTERVAL '1d'"
    elif time_range_seconds < 604800:  # < 1 week
        # Query 1-hour rollup
        query = "SELECT bucket, avg_cpu FROM metrics_1hour WHERE bucket > NOW() - INTERVAL '7d'"
    else:
        # Query daily rollup
        query = "SELECT bucket, avg_cpu FROM metrics_daily WHERE bucket > NOW() - INTERVAL '30d'"

    data = execute_query(query)

    # LTTB downsample to 800 points if still too many
    if len(data) > 800:
        data = lttb(data, 800)

    return data
```

## Performance Benchmarks

### LTTB Performance

| Dataset Size | Threshold | Python Time | TypeScript Time | SQL Time (TimescaleDB) |
|--------------|-----------|-------------|-----------------|------------------------|
| 10K points   | 1,000     | 15ms        | 20ms            | 50ms                   |
| 100K points  | 1,000     | 150ms       | 200ms           | 500ms                  |
| 1M points    | 1,000     | 1.5s        | 2s              | 5s                     |

**Recommendation**: Pre-aggregate with continuous aggregates for > 100K points, then apply LTTB.

### Memory Usage

| Dataset Size | Raw JSON | LTTB (1,000 points) | Savings |
|--------------|----------|---------------------|---------|
| 10K points   | 200KB    | 20KB                | 90%     |
| 100K points  | 2MB      | 20KB                | 99%     |
| 1M points    | 20MB     | 20KB                | 99.9%   |

## Best Practices

1. **Target Points**: Use 500-1000 points for most charts (1 point per pixel max)
2. **Time Range Strategy**: Short ranges (< 1h) use raw data, long ranges use rollups + LTTB
3. **Server-Side Downsampling**: Never send 100K+ points to frontend
4. **Caching**: Cache downsampled results for 30-60 seconds
5. **Adaptive Thresholds**: Adjust points based on time range and chart width
6. **LTTB over Averaging**: LTTB preserves visual fidelity better than averaging
7. **Pre-aggregation**: Use continuous aggregates for long time ranges
8. **API Design**: Let client specify target points for flexibility
9. **Progressive Loading**: Load low-resolution first, then stream high-resolution
10. **Monitoring**: Track downsampling time and cache hit rates

## Example: Complete FastAPI Integration

```python
from fastapi import FastAPI, Query
from typing import List, Tuple
import psycopg2

app = FastAPI()

def lttb(data: List[Tuple], threshold: int) -> List[Tuple]:
    # ... (implementation from above)
    pass

def get_db_connection():
    return psycopg2.connect(
        host="localhost",
        database="postgres",
        user="postgres",
        password="password"
    )

@app.get("/api/metrics/{metric}")
async def get_metric(
    metric: str,
    start: str = Query(..., description="Time range: 1h, 1d, 7d, 30d"),
    points: int = Query(800, ge=100, le=2000, description="Target number of points")
):
    # Parse time range
    time_ranges = {
        "1h": ("1 hour", "metrics", "time"),
        "1d": ("1 day", "metrics_1min", "bucket"),
        "7d": ("7 days", "metrics_1hour", "bucket"),
        "30d": ("30 days", "metrics_daily", "bucket")
    }

    interval, table, time_col = time_ranges.get(start, ("1 hour", "metrics", "time"))

    # Query database
    conn = get_db_connection()
    cur = conn.cursor()

    cur.execute(f"""
        SELECT {time_col}, {metric}
        FROM {table}
        WHERE {time_col} > NOW() - INTERVAL '{interval}'
        ORDER BY {time_col} ASC
    """)

    data = cur.fetchall()
    cur.close()
    conn.close()

    # Downsample if needed
    if len(data) > points:
        data = lttb(data, points)

    # Convert to JSON-serializable format
    return [{"time": row[0].isoformat(), "value": row[1]} for row in data]
```

## Resources

- Original LTTB paper: https://skemman.is/bitstream/1946/15343/3/SS_MSthesis.pdf
- TimescaleDB Toolkit: https://docs.timescale.com/timescaledb/latest/how-to-guides/hyperfunctions/function-pipelines/
- Flot downsampling plugin: https://github.com/sveinn-steinarsson/flot-downsample
