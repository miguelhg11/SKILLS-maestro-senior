# Data Pipeline Blueprint

**Version:** 1.0.0
**Last Updated:** 2025-12-06
**Category:** Data

---

## Overview

Pre-configured skill chain optimized for building production-grade data pipelines, ETL workflows, streaming data architectures, and batch processing systems. This blueprint provides battle-tested defaults for the most common data engineering patterns, minimizing configuration while maximizing reliability and performance.

---

## Trigger Keywords

**Primary (high confidence):**
- data pipeline
- etl
- data ingestion
- data processing
- streaming
- batch processing

**Secondary (medium confidence):**
- kafka
- spark
- airflow
- data warehouse
- data lake
- data transformation
- data integration
- data flow

**Example goals that match:**
- "ETL pipeline from PostgreSQL to Snowflake"
- "real-time streaming pipeline with Kafka"
- "batch processing pipeline for analytics"
- "data ingestion from APIs to data warehouse"
- "build data pipeline with validation and monitoring"
- "streaming data processing with transformations"

---

## Skill Chain (Pre-configured)

This blueprint invokes 6 skills in the following order:

```
1. designing-using-relational-databases   (data modeling - source/destination schemas)
2. streaming-data                   (real-time processing architecture)
3. optimizing-sql                   (query performance and transformations)
4. testing-strategies               (data quality and validation tests)
5. implementing-observability       (pipeline monitoring and alerting)
6. writing-documentation            (pipeline documentation and runbooks)
```

**Total estimated time:** 35-45 minutes
**Total estimated questions:** 10-15 questions (or 3 with blueprint defaults)

---

## Pre-configured Defaults

### 1. designing-using-relational-databases
```yaml
schema_design: "star-schema"
  # Optimized for analytics workloads
  # Fact tables: Transactional events, metrics
  # Dimension tables: Reference data, lookups
  # Supports efficient JOIN operations

normalization: "3NF-source-denormalized-target"
  # Source: Normalized (3NF) for data integrity
  # Target: Denormalized (star/snowflake) for query performance
  # Balance between consistency and speed

partitioning_strategy: "time-based"
  # Partition by date/timestamp (daily or monthly)
  # Enables efficient incremental processing
  # Supports retention policies and archival

indexes: "selective"
  # Primary keys: Clustered indexes on fact tables
  # Foreign keys: Non-clustered indexes on dimension tables
  # Composite indexes for common query patterns
  # Avoid over-indexing (impacts write performance)

data_types: "strict"
  # Enforce explicit data types (no VARCHAR(MAX))
  # Use appropriate precision (DECIMAL for money)
  # Timestamp with timezone for temporal data
  # JSON/JSONB for semi-structured data
```

### 2. streaming-data
```yaml
architecture: "kappa"
  # Single stream processing layer (no batch layer)
  # Real-time processing with replayable event log
  # Simpler than Lambda architecture for most use cases
  # Kafka or Kinesis as event backbone

processing_model: "micro-batching"
  # Balance between latency and throughput
  # Default window: 5 seconds
  # Enables aggregations and windowing operations
  # Better resource utilization than per-event processing

backpressure_handling: "dynamic-scaling"
  # Auto-scale consumers based on lag
  # Rate limiting on producers
  # Dead letter queue for failed messages
  # Circuit breaker for downstream failures

message_format: "avro"
  # Schema evolution support
  # Compact binary format
  # Schema registry for version management
  # Better than JSON for high-volume streams

exactly_once_semantics: true
  # Idempotent producers
  # Transactional consumers
  # Deduplication at sink
  # Critical for financial/billing data

watermarking: "event-time"
  # Use event timestamp (not processing time)
  # Handle late-arriving data
  # Configurable allowed lateness (default: 1 hour)
  # Ensures correct windowing and aggregations
```

### 3. optimizing-sql
```yaml
query_optimization: "explain-plan-driven"
  # Analyze execution plans for all queries
  # Identify sequential scans, missing indexes
  # Optimize JOIN order and conditions
  # Use CTEs for readability, subqueries for performance

materialized_views: true
  # Pre-compute expensive aggregations
  # Refresh strategy: Incremental (not full refresh)
  # Index materialized views for query performance
  # Balance freshness vs. compute cost

partitioning: "range-and-hash"
  # Range partitioning on timestamp (primary)
  # Hash partitioning on high-cardinality keys (secondary)
  # Enables partition pruning in queries
  # Supports parallel processing

batch_operations: true
  # Bulk INSERT/UPDATE over row-by-row
  # Use COPY for large data loads
  # Batch DELETE with chunking (avoid locks)
  # Transaction batching for commit efficiency

connection_pooling: true
  # Pool size: 10-50 connections (based on load)
  # Connection timeout: 30 seconds
  # Idle connection timeout: 5 minutes
  # Prevents connection exhaustion
```

### 4. testing-strategies
```yaml
data_quality_tests: "great-expectations"
  # Expectation suites for each data source
  # Validate schema, completeness, uniqueness, ranges
  # Run on every pipeline execution
  # Fail pipeline on critical violations

test_levels:
  unit: "transformation-functions"
    # Test individual data transformations
    # Pure functions, deterministic outputs
    # Mock data sources and sinks

  integration: "end-to-end-pipeline"
    # Test full pipeline with sample data
    # Validate source â†’ transform â†’ sink flow
    # Check idempotency and error handling

  data_quality: "production-data-sampling"
    # Sample recent production data
    # Run validation rules
    # Monitor quality metrics over time

validation_rules:
  - schema_validation: "strict"
      # Column names, types, nullability
      # Fail on schema drift

  - completeness: "required-fields"
      # No nulls in non-nullable columns
      # Row count expectations

  - uniqueness: "primary-keys"
      # Duplicate detection
      # Composite key validation

  - freshness: "max-staleness"
      # Data arrival SLAs
      # Alert if data delayed >1 hour

  - referential_integrity: "foreign-keys"
      # Validate JOIN integrity
      # Orphaned record detection

test_data: "production-snapshot"
  # Use anonymized production data
  # Realistic data distributions
  # Include edge cases and outliers
```

### 5. implementing-observability
```yaml
metrics: "prometheus-format"
  # Pipeline-level metrics:
    # - Records processed per second
    # - Processing latency (p50, p95, p99)
    # - Error rate (errors per 1000 records)
    # - Backlog/lag (for streaming)
    # - Resource utilization (CPU, memory, disk)

  # Data quality metrics:
    # - Validation failures
    # - Schema violations
    # - Null rate by column
    # - Duplicate rate

logging: "structured-json"
  # JSON format for easy parsing
  # Log levels: DEBUG, INFO, WARN, ERROR, CRITICAL
  # Include correlation IDs for request tracing
  # Log sampling for high-volume events (1% of INFO)

tracing: "distributed"
  # OpenTelemetry for trace propagation
  # Trace each record through pipeline stages
  # Identify bottlenecks and latency spikes
  # Visualize in Jaeger or Zipkin

alerting: "threshold-and-anomaly"
  # Threshold alerts:
    # - Error rate >1%
    # - Processing lag >5 minutes
    # - Data freshness >1 hour
    # - Validation failure rate >5%

  # Anomaly detection:
    # - Sudden drop in throughput
    # - Unusual data patterns
    # - Resource spikes

alert_channels:
  critical: "pagerduty"
    # Pipeline failures
    # Data loss events
    # SLA violations

  warning: "slack"
    # Performance degradation
    # Validation warnings
    # Resource alerts

  info: "dashboard"
    # Daily summaries
    # Trend analysis

dashboards: "grafana"
  # Real-time pipeline health
  # Historical performance trends
  # Data quality scorecards
  # Resource utilization
  # Alert status
```

### 6. writing-documentation
```yaml
documentation_types:
  architecture: "pipeline-diagram"
    # Data flow visualization
    # Component dependencies
    # Technology stack
    # Deployment topology

  runbook: "operational-procedures"
    # Deployment steps
    # Rollback procedures
    # Troubleshooting guides
    # Common failure modes

  data_dictionary: "schema-catalog"
    # Table and column descriptions
    # Data lineage
    # Sample values
    # Business rules

  sla: "service-level-objectives"
    # Latency targets
    # Freshness guarantees
    # Availability requirements
    # Error budgets

format: "markdown-with-diagrams"
  # Markdown for readability
  # Mermaid diagrams for data flow
  # Code blocks for SQL/config
  # Searchable and version-controlled

maintenance: "living-documentation"
  # Update with code changes
  # Review quarterly
  # Include in PR checklist
  # Auto-generate from code where possible
```

---

## Quick Questions (Only 3)

When blueprint is detected, ask only these essential questions:

### Question 1: Processing Type
```
What type of data processing do you need?

Options:
1. Batch processing (scheduled runs, large volumes)
2. Stream processing (real-time, continuous)
3. Hybrid (batch + streaming - Lambda/Kappa architecture)

Your answer: _______________
```

**Why this matters:**
- Determines orchestration tool (Airflow vs Kafka/Flink)
- Affects architecture complexity
- Influences resource provisioning
- Changes monitoring requirements

**Default if skipped:** "Batch processing (Airflow-based)"

---

### Question 2: Data Volume
```
What is your expected data volume?

Options:
1. Small (<10 GB/day, <100K records/hour)
2. Medium (10-100 GB/day, 100K-1M records/hour)
3. Large (100+ GB/day, 1M+ records/hour)

Your answer: _______________
```

**Why this matters:**
- Small: Single-node processing (Pandas, DuckDB)
- Medium: Distributed processing optional (Spark on small cluster)
- Large: Distributed processing required (Spark, Flink)
- Affects infrastructure costs and complexity

**Default if skipped:** "Medium (10-100 GB/day)"

---

### Question 3: Primary Destination
```
Where will the processed data be stored?

Options:
1. Data warehouse (Snowflake, BigQuery, Redshift)
2. Data lake (S3/Parquet, Delta Lake, Iceberg)
3. OLTP database (PostgreSQL, MySQL)
4. Analytics platform (Elasticsearch, ClickHouse)

Your answer: _______________
```

**Why this matters:**
- Determines data format (Parquet, JSON, CSV)
- Affects partitioning strategy
- Influences schema design
- Changes optimization approaches

**Default if skipped:** "Data warehouse (Snowflake/BigQuery)"

---

## Blueprint Detection Logic

Trigger this blueprint when the user's goal matches:

```python
def should_use_data_pipeline_blueprint(goal: str) -> bool:
    goal_lower = goal.lower()

    # Primary keywords (high confidence)
    primary_match = any(keyword in goal_lower for keyword in [
        "data pipeline",
        "etl",
        "data ingestion",
        "data processing",
        "streaming",
        "batch processing"
    ])

    # Secondary keywords + data movement
    secondary_match = (
        any(keyword in goal_lower for keyword in [
            "kafka", "spark", "airflow", "flink", "beam"
        ]) and
        any(keyword in goal_lower for keyword in [
            "data", "pipeline", "processing", "streaming", "batch"
        ])
    )

    # Data integration pattern
    integration_match = (
        any(keyword in goal_lower for keyword in [
            "from", "to", "into", "load", "extract", "transform"
        ]) and
        any(keyword in goal_lower for keyword in [
            "warehouse", "database", "lake", "postgres", "s3", "snowflake"
        ])
    )

    return primary_match or secondary_match or integration_match
```

**Confidence levels:**
- **High (90%+):** Contains "data pipeline" or "ETL"
- **Medium (70-89%):** Contains data technology + processing term
- **Low (50-69%):** Contains data integration pattern

**Present blueprint when confidence >= 70%**

---

## Blueprint Offer Message

When blueprint is detected, present this message to the user:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”„ DATA PIPELINE BLUEPRINT DETECTED                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Your goal matches our optimized Data Pipeline Blueprint!  â”‚
â”‚                                                            â”‚
â”‚ Pre-configured features:                                   â”‚
â”‚  âœ“ Battle-tested architecture (batch/streaming/hybrid)    â”‚
â”‚  âœ“ Data quality validation (Great Expectations)           â”‚
â”‚  âœ“ Schema design (star schema for analytics)              â”‚
â”‚  âœ“ Query optimization (indexes, partitions, views)        â”‚
â”‚  âœ“ Comprehensive monitoring (metrics, logs, traces)       â”‚
â”‚  âœ“ Error handling (retries, dead letter queue)            â”‚
â”‚  âœ“ Production-ready (idempotency, exactly-once)           â”‚
â”‚                                                            â”‚
â”‚ Using blueprint reduces questions from 15 to 3!           â”‚
â”‚                                                            â”‚
â”‚ Options:                                                   â”‚
â”‚  1. Use blueprint (3 quick questions, ~10 min)            â”‚
â”‚  2. Custom configuration (15 questions, ~40 min)          â”‚
â”‚  3. Skip all questions (use all defaults, ~5 min)         â”‚
â”‚                                                            â”‚
â”‚ Your choice (1/2/3): _____                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Handle responses:**
- **1 or "blueprint"** â†’ Ask only 3 blueprint questions
- **2 or "custom"** â†’ Ask all skill questions (normal flow)
- **3 or "skip"** â†’ Use all defaults, skip all questions

---

## Generated Output Structure

When blueprint is executed, generate this file structure:

```
data-pipeline-project/
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ pipeline.yaml               # Main pipeline configuration
â”‚   â”‚   â”œâ”€â”€ connections.yaml            # Source/destination configs
â”‚   â”‚   â”œâ”€â”€ schema_definitions.yaml     # Data schemas and contracts
â”‚   â”‚   â””â”€â”€ quality_expectations.yaml   # Data quality rules
â”‚   â”‚
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”œâ”€â”€ sources/
â”‚   â”‚   â”‚   â”œâ”€â”€ database_extractor.py   # SQL database extraction
â”‚   â”‚   â”‚   â”œâ”€â”€ api_extractor.py        # REST API extraction
â”‚   â”‚   â”‚   â”œâ”€â”€ file_extractor.py       # CSV/JSON/Parquet files
â”‚   â”‚   â”‚   â””â”€â”€ stream_consumer.py      # Kafka/Kinesis consumer
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”‚       â”œâ”€â”€ source_schema.py        # Source data models
â”‚   â”‚       â””â”€â”€ validation.py           # Input validation
â”‚   â”‚
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â”œâ”€â”€ transformations/
â”‚   â”‚   â”‚   â”œâ”€â”€ cleansing.py            # Data cleaning functions
â”‚   â”‚   â”‚   â”œâ”€â”€ enrichment.py           # Data enrichment
â”‚   â”‚   â”‚   â”œâ”€â”€ aggregations.py         # Aggregation logic
â”‚   â”‚   â”‚   â””â”€â”€ business_rules.py       # Business logic
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ sql/
â”‚   â”‚   â”‚   â”œâ”€â”€ staging_tables.sql      # Staging table DDL
â”‚   â”‚   â”‚   â”œâ”€â”€ transformations.sql     # SQL transformations
â”‚   â”‚   â”‚   â””â”€â”€ materialized_views.sql  # Pre-computed views
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”‚       â”œâ”€â”€ intermediate_schema.py  # Intermediate models
â”‚   â”‚       â””â”€â”€ target_schema.py        # Final output models
â”‚   â”‚
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â”œâ”€â”€ sinks/
â”‚   â”‚   â”‚   â”œâ”€â”€ warehouse_loader.py     # Data warehouse loading
â”‚   â”‚   â”‚   â”œâ”€â”€ database_loader.py      # Database INSERT/UPDATE
â”‚   â”‚   â”‚   â”œâ”€â”€ file_writer.py          # Parquet/CSV output
â”‚   â”‚   â”‚   â””â”€â”€ stream_producer.py      # Kafka/Kinesis producer
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ strategies/
â”‚   â”‚   â”‚   â”œâ”€â”€ full_refresh.py         # Truncate and load
â”‚   â”‚   â”‚   â”œâ”€â”€ incremental.py          # Append new records
â”‚   â”‚   â”‚   â”œâ”€â”€ upsert.py               # Merge/update logic
â”‚   â”‚   â”‚   â””â”€â”€ scd_type2.py            # Slowly changing dimensions
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ partitioning/
â”‚   â”‚       â”œâ”€â”€ time_partitions.py      # Date-based partitioning
â”‚   â”‚       â””â”€â”€ hash_partitions.py      # Key-based partitioning
â”‚   â”‚
â”‚   â”œâ”€â”€ quality/
â”‚   â”‚   â”œâ”€â”€ expectations/
â”‚   â”‚   â”‚   â”œâ”€â”€ source_expectations.py  # Source data validation
â”‚   â”‚   â”‚   â”œâ”€â”€ transform_expectations.py # Mid-pipeline checks
â”‚   â”‚   â”‚   â””â”€â”€ output_expectations.py  # Final validation
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ validators/
â”‚   â”‚   â”‚   â”œâ”€â”€ schema_validator.py     # Schema compliance
â”‚   â”‚   â”‚   â”œâ”€â”€ completeness_validator.py # Null checks
â”‚   â”‚   â”‚   â”œâ”€â”€ uniqueness_validator.py # Duplicate detection
â”‚   â”‚   â”‚   â””â”€â”€ freshness_validator.py  # Timeliness checks
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ reports/
â”‚   â”‚       â”œâ”€â”€ quality_report.py       # Generate reports
â”‚   â”‚       â””â”€â”€ data_profiling.py       # Data profiling
â”‚   â”‚
â”‚   â”œâ”€â”€ orchestration/
â”‚   â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_pipeline.py   # Batch DAG definition
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ monitoring_dag.py   # Health check DAG
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ operators/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ custom_operators.py # Custom Airflow operators
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ sensors.py          # Custom sensors
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ plugins/
â”‚   â”‚   â”‚       â””â”€â”€ pipeline_plugin.py  # Airflow plugins
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ streaming/
â”‚   â”‚       â”œâ”€â”€ kafka_streams.py        # Kafka Streams app
â”‚   â”‚       â”œâ”€â”€ flink_job.py            # Flink job definition
â”‚   â”‚       â””â”€â”€ consumer_groups.py      # Consumer group config
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline_metrics.py     # Custom metrics
â”‚   â”‚   â”‚   â”œâ”€â”€ prometheus_exporter.py  # Prometheus exporter
â”‚   â”‚   â”‚   â””â”€â”€ metric_definitions.yaml # Metric catalog
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ logging/
â”‚   â”‚   â”‚   â”œâ”€â”€ logger.py               # Structured logging
â”‚   â”‚   â”‚   â”œâ”€â”€ log_config.yaml         # Logging configuration
â”‚   â”‚   â”‚   â””â”€â”€ correlation.py          # Trace correlation
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ alerts/
â”‚   â”‚   â”‚   â”œâ”€â”€ alert_rules.yaml        # Alert definitions
â”‚   â”‚   â”‚   â”œâ”€â”€ pagerduty_config.yaml   # PagerDuty integration
â”‚   â”‚   â”‚   â””â”€â”€ slack_notifier.py       # Slack notifications
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â”œâ”€â”€ grafana/
â”‚   â”‚       â”‚   â”œâ”€â”€ pipeline_dashboard.json # Main dashboard
â”‚   â”‚       â”‚   â””â”€â”€ quality_dashboard.json  # Data quality
â”‚   â”‚       â”‚
â”‚   â”‚       â””â”€â”€ templates/
â”‚   â”‚           â””â”€â”€ dashboard_template.py   # Dashboard as code
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ connection_pool.py          # Database connection pooling
â”‚   â”‚   â”œâ”€â”€ retry_logic.py              # Retry/backoff strategies
â”‚   â”‚   â”œâ”€â”€ error_handling.py           # Error handlers
â”‚   â”‚   â”œâ”€â”€ deduplication.py            # Dedup utilities
â”‚   â”‚   â””â”€â”€ watermarking.py             # Event-time watermarks
â”‚   â”‚
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ unit/
â”‚       â”‚   â”œâ”€â”€ test_transformations.py # Transformation tests
â”‚       â”‚   â”œâ”€â”€ test_validators.py      # Validation tests
â”‚       â”‚   â””â”€â”€ test_utils.py           # Utility tests
â”‚       â”‚
â”‚       â”œâ”€â”€ integration/
â”‚       â”‚   â”œâ”€â”€ test_extract.py         # Extraction tests
â”‚       â”‚   â”œâ”€â”€ test_load.py            # Loading tests
â”‚       â”‚   â””â”€â”€ test_end_to_end.py      # Full pipeline tests
â”‚       â”‚
â”‚       â””â”€â”€ fixtures/
â”‚           â”œâ”€â”€ sample_data.json        # Test data
â”‚           â””â”€â”€ expected_outputs.json   # Expected results
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â”œâ”€â”€ main.tf                     # Infrastructure as code
â”‚   â”‚   â”œâ”€â”€ variables.tf                # Configuration variables
â”‚   â”‚   â”œâ”€â”€ outputs.tf                  # Output values
â”‚   â”‚   â””â”€â”€ modules/
â”‚   â”‚       â”œâ”€â”€ kafka_cluster/          # Kafka infrastructure
â”‚   â”‚       â”œâ”€â”€ database/               # Database setup
â”‚   â”‚       â””â”€â”€ monitoring/             # Monitoring stack
â”‚   â”‚
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile                  # Pipeline container
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml          # Local development stack
â”‚   â”‚   â””â”€â”€ airflow.Dockerfile          # Airflow container
â”‚   â”‚
â”‚   â””â”€â”€ kubernetes/
â”‚       â”œâ”€â”€ deployment.yaml             # K8s deployment
â”‚       â”œâ”€â”€ service.yaml                # Service definition
â”‚       â””â”€â”€ configmap.yaml              # Configuration
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture/
â”‚   â”‚   â”œâ”€â”€ data_flow_diagram.md        # Data flow visualization
â”‚   â”‚   â”œâ”€â”€ component_diagram.md        # Component architecture
â”‚   â”‚   â””â”€â”€ technology_stack.md         # Tech stack details
â”‚   â”‚
â”‚   â”œâ”€â”€ runbooks/
â”‚   â”‚   â”œâ”€â”€ deployment.md               # Deployment procedures
â”‚   â”‚   â”œâ”€â”€ rollback.md                 # Rollback guide
â”‚   â”‚   â”œâ”€â”€ troubleshooting.md          # Common issues
â”‚   â”‚   â””â”€â”€ disaster_recovery.md        # DR procedures
â”‚   â”‚
â”‚   â”œâ”€â”€ data_catalog/
â”‚   â”‚   â”œâ”€â”€ data_dictionary.md          # Table/column docs
â”‚   â”‚   â”œâ”€â”€ lineage.md                  # Data lineage
â”‚   â”‚   â””â”€â”€ sample_queries.md           # Example queries
â”‚   â”‚
â”‚   â””â”€â”€ sla/
â”‚       â”œâ”€â”€ service_levels.md           # SLA definitions
â”‚       â””â”€â”€ error_budgets.md            # Error budget tracking
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_database.sh               # Database initialization
â”‚   â”œâ”€â”€ backfill_data.py                # Historical data backfill
â”‚   â”œâ”€â”€ validate_pipeline.py            # Pipeline validation
â”‚   â””â”€â”€ generate_sample_data.py         # Test data generation
â”‚
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ setup.py                            # Package setup
â”œâ”€â”€ pyproject.toml                      # Poetry/modern setup
â”œâ”€â”€ .env.example                        # Environment variables template
â”œâ”€â”€ .gitignore                          # Git ignore rules
â”œâ”€â”€ Makefile                            # Common commands
â””â”€â”€ README.md                           # Project overview
```

---

## Pipeline Architecture Patterns

### Batch Processing Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BATCH PIPELINE (AIRFLOW)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Extract  â”‚â”€â”€â†’â”‚Transform â”‚â”€â”€â†’â”‚  Load    â”‚â”€â”€â†’â”‚Validate â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚              â”‚              â”‚              â”‚        â”‚
â”‚  [Source DB]    [Staging]    [Data Warehouse]  [Quality]   â”‚
â”‚       â”‚              â”‚              â”‚              â”‚        â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         Airflow DAG (Scheduled Execution)             â”‚ â”‚
â”‚  â”‚  - Extract: 02:00 UTC daily                           â”‚ â”‚
â”‚  â”‚  - Transform: On extract success                      â”‚ â”‚
â”‚  â”‚  - Load: On transform success                         â”‚ â”‚
â”‚  â”‚  - Validate: On load success                          â”‚ â”‚
â”‚  â”‚  - Retry: 3 attempts with exponential backoff         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              MONITORING & ALERTING                    â”‚ â”‚
â”‚  â”‚  - Metrics: Prometheus (records/sec, errors)          â”‚ â”‚
â”‚  â”‚  - Logs: Structured JSON to CloudWatch/ELK           â”‚ â”‚
â”‚  â”‚  - Alerts: PagerDuty (failures), Slack (warnings)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Streaming Architecture (Kappa)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               STREAMING PIPELINE (KAFKA + FLINK)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚   Producers  â”‚ (APIs, DBs, Apps)                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              KAFKA TOPIC (raw-events)               â”‚   â”‚
â”‚  â”‚  - Partitions: 10 (for parallelism)                â”‚   â”‚
â”‚  â”‚  - Replication: 3 (for durability)                 â”‚   â”‚
â”‚  â”‚  - Retention: 7 days                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                 â”‚
â”‚           â–¼                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         FLINK STREAM PROCESSING                    â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚    â”‚
â”‚  â”‚  â”‚ Validate â”‚â”€â†’â”‚Transform â”‚â”€â†’â”‚Aggregate â”‚        â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚    â”‚
â”‚  â”‚  - Window: 5-second tumbling                      â”‚    â”‚
â”‚  â”‚  - Watermark: 1-hour allowed lateness             â”‚    â”‚
â”‚  â”‚  - State: RocksDB backend                         â”‚    â”‚
â”‚  â”‚  - Checkpoints: Every 60 seconds                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜    â”‚
â”‚           â”‚                                         â”‚       â”‚
â”‚           â–¼                                         â–¼       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ KAFKA TOPIC    â”‚                    â”‚ DEAD LETTER      â”‚â”‚
â”‚  â”‚ (processed)    â”‚                    â”‚ QUEUE (errors)   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚           â”‚                                                 â”‚
â”‚           â–¼                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚          SINK CONNECTORS                           â”‚    â”‚
â”‚  â”‚  - Snowflake Connector (analytics)                 â”‚    â”‚
â”‚  â”‚  - S3 Sink (data lake)                             â”‚    â”‚
â”‚  â”‚  - Elasticsearch (search)                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         MONITORING (REAL-TIME)                        â”‚ â”‚
â”‚  â”‚  - Consumer lag: <5 minutes                           â”‚ â”‚
â”‚  â”‚  - Throughput: Records/sec per partition             â”‚ â”‚
â”‚  â”‚  - Error rate: Failed messages/total                 â”‚ â”‚
â”‚  â”‚  - Latency: End-to-end processing time               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hybrid Architecture (Lambda)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         HYBRID PIPELINE (LAMBDA ARCHITECTURE)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                     â”‚ Data Sources â”‚                        â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                             â”‚                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚              â”‚                             â”‚                â”‚
â”‚              â–¼                             â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   BATCH LAYER       â”‚      â”‚   SPEED LAYER           â”‚  â”‚
â”‚  â”‚   (Spark)           â”‚      â”‚   (Flink/Kafka Streams) â”‚  â”‚
â”‚  â”‚                     â”‚      â”‚                         â”‚  â”‚
â”‚  â”‚  - Full history     â”‚      â”‚  - Recent data only     â”‚  â”‚
â”‚  â”‚  - High latency     â”‚      â”‚  - Low latency          â”‚  â”‚
â”‚  â”‚  - High accuracy    â”‚      â”‚  - Approximate results  â”‚  â”‚
â”‚  â”‚  - Runs: Daily      â”‚      â”‚  - Runs: Continuous     â”‚  â”‚
â”‚  â”‚  - Recomputes all   â”‚      â”‚  - Incremental updates  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                            â”‚                  â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                       â”‚                                     â”‚
â”‚                       â–¼                                     â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚            â”‚   SERVING LAYER    â”‚                           â”‚
â”‚            â”‚   (Data Warehouse) â”‚                           â”‚
â”‚            â”‚                    â”‚                           â”‚
â”‚            â”‚  - Batch Views     â”‚                           â”‚
â”‚            â”‚  - Real-time Views â”‚                           â”‚
â”‚            â”‚  - Merged queries  â”‚                           â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                             â”‚
â”‚  Use case: Financial reporting with real-time dashboard    â”‚
â”‚  - Batch: Official daily reports (accurate)                â”‚
â”‚  - Speed: Live dashboard updates (approximate)             â”‚
â”‚  - Serving: Query merges both views                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Quality Framework

### Validation Checkpoints

```yaml
checkpoint_1_source_validation:
  location: "After extraction, before transformation"
  rules:
    - schema_compliance:
        description: "Validate column names, types, nullability"
        severity: "critical"
        action: "fail_pipeline"

    - row_count:
        description: "Expected range: 1000-100000 records"
        severity: "warning"
        action: "alert_only"

    - freshness:
        description: "Data must be <24 hours old"
        severity: "critical"
        action: "fail_pipeline"

checkpoint_2_transformation_validation:
  location: "After transformations, before loading"
  rules:
    - completeness:
        description: "Required columns must be non-null"
        columns: ["user_id", "timestamp", "event_type"]
        severity: "critical"
        action: "fail_pipeline"

    - uniqueness:
        description: "Primary key must be unique"
        columns: ["transaction_id"]
        severity: "critical"
        action: "fail_pipeline"

    - value_ranges:
        description: "Amount must be between 0 and 1000000"
        column: "amount"
        min: 0
        max: 1000000
        severity: "warning"
        action: "log_violations"

checkpoint_3_output_validation:
  location: "After loading to destination"
  rules:
    - referential_integrity:
        description: "Foreign keys must exist in dimension tables"
        severity: "critical"
        action: "fail_pipeline"

    - reconciliation:
        description: "Row count matches between source and destination"
        tolerance: "0.01%"  # Allow 0.01% discrepancy
        severity: "critical"
        action: "fail_pipeline"

    - business_rules:
        description: "Total revenue must be positive"
        custom_sql: "SELECT SUM(revenue) > 0 FROM fact_sales"
        severity: "warning"
        action: "alert_only"
```

---

## Error Handling Strategies

### Retry Configuration

```yaml
retry_policies:
  transient_errors:
    description: "Network timeouts, temporary unavailability"
    max_attempts: 3
    backoff_strategy: "exponential"
    initial_delay: "5s"
    max_delay: "60s"
    multiplier: 2
    jitter: true

  data_quality_failures:
    description: "Validation failures, schema violations"
    max_attempts: 1
    action: "send_to_dead_letter_queue"
    alert: "pagerduty"

  resource_exhaustion:
    description: "Out of memory, connection pool exhausted"
    max_attempts: 2
    backoff_strategy: "fixed"
    delay: "30s"
    action: "reduce_batch_size_and_retry"

dead_letter_queue:
  enabled: true
  retention: "30 days"
  processing:
    - manual_review: "Data quality team"
    - automated_retry: "After source data fix"
    - discard: "After 30 days"
```

---

## Performance Optimization

### Batch Processing Optimizations

```yaml
optimizations:
  parallelization:
    - partition_data: "Process data in parallel chunks"
    - chunk_size: "10000 records"
    - max_workers: "CPU count * 2"

  incremental_loading:
    - strategy: "Change Data Capture (CDC)"
    - watermark_column: "updated_at"
    - checkpoint: "Store last processed timestamp"
    - fallback: "Full refresh if checkpoint missing"

  compression:
    - format: "Parquet with Snappy compression"
    - compression_ratio: "~10:1 for typical data"
    - benefits: "Reduced I/O, faster queries"

  caching:
    - dimension_tables: "Cache in memory (small tables)"
    - lookup_tables: "Redis cache for frequent lookups"
    - ttl: "1 hour"
```

### Streaming Optimizations

```yaml
optimizations:
  batching:
    - micro_batch_size: "1000 records or 5 seconds"
    - benefits: "Higher throughput, lower overhead"

  state_management:
    - backend: "RocksDB (persistent, fast)"
    - checkpointing: "Every 60 seconds"
    - state_ttl: "7 days for inactive keys"

  partitioning:
    - strategy: "Hash partitioning on user_id"
    - partition_count: "10 (matches Kafka partitions)"
    - benefits: "Parallel processing, even distribution"

  backpressure:
    - detection: "Monitor consumer lag"
    - action: "Scale consumers horizontally"
    - threshold: "Lag > 5 minutes"
```

---

## Dependencies

The blueprint includes these Python packages:

```toml
[tool.poetry.dependencies]
python = "^3.10"

# Core data processing
pandas = "^2.1.0"                  # Data manipulation
polars = "^0.19.0"                 # Fast DataFrame library (alternative to pandas)
duckdb = "^0.9.0"                  # In-process SQL OLAP database

# Distributed processing (optional - based on volume)
pyspark = "^3.5.0"                 # Batch processing (medium/large data)
apache-flink = "^1.18.0"           # Stream processing

# Orchestration
apache-airflow = "^2.7.0"          # Batch orchestration
apache-airflow-providers-postgres = "^5.7.0"
apache-airflow-providers-snowflake = "^5.1.0"

# Streaming
kafka-python = "^2.0.2"            # Kafka client
confluent-kafka = "^2.3.0"         # Confluent Kafka (better performance)
fastavro = "^1.9.0"                # Avro serialization

# Data quality
great-expectations = "^0.18.0"     # Data validation
pydantic = "^2.5.0"                # Data models and validation

# Database connectors
psycopg2-binary = "^2.9.0"         # PostgreSQL
pymysql = "^1.1.0"                 # MySQL
snowflake-connector-python = "^3.4.0"  # Snowflake
google-cloud-bigquery = "^3.13.0"  # BigQuery

# Storage
boto3 = "^1.29.0"                  # AWS S3
pyarrow = "^14.0.0"                # Parquet files

# Monitoring
prometheus-client = "^0.19.0"      # Metrics
opentelemetry-api = "^1.21.0"      # Distributed tracing
structlog = "^23.2.0"              # Structured logging

# Utilities
tenacity = "^8.2.0"                # Retry logic
python-dotenv = "^1.0.0"           # Environment variables
pyyaml = "^6.0.0"                  # YAML parsing
```

**Infrastructure dependencies:**
- Airflow: Orchestration for batch pipelines
- Kafka: Event streaming platform
- PostgreSQL: Metadata storage
- Redis: Caching and state management
- Prometheus + Grafana: Monitoring
- Docker + Kubernetes: Containerization

---

## Monitoring Dashboards

### Pipeline Health Dashboard

```yaml
dashboard: "Pipeline Health"
panels:
  - title: "Pipeline Status"
    type: "stat"
    metrics:
      - current_status: "Success | Failed | Running"
      - last_run_time: "2024-12-06 02:00 UTC"
      - next_scheduled_run: "2024-12-07 02:00 UTC"

  - title: "Processing Rate"
    type: "graph"
    metrics:
      - records_per_second: "Gauge"
      - total_records_processed: "Counter"
      - processing_duration: "Histogram"

  - title: "Error Rate"
    type: "graph"
    metrics:
      - errors_per_minute: "Rate of failed records"
      - error_percentage: "Errors / total records"
    thresholds:
      - warning: ">0.5%"
      - critical: ">1.0%"

  - title: "Data Quality"
    type: "table"
    metrics:
      - validation_pass_rate: "Percentage"
      - schema_violations: "Count"
      - null_rate_by_column: "Percentage per column"

  - title: "Resource Utilization"
    type: "graph"
    metrics:
      - cpu_usage: "Percentage"
      - memory_usage: "Percentage"
      - disk_io: "MB/s"

  - title: "Latency"
    type: "graph"
    metrics:
      - p50_latency: "Median processing time"
      - p95_latency: "95th percentile"
      - p99_latency: "99th percentile"
```

### Data Quality Dashboard

```yaml
dashboard: "Data Quality Scorecard"
panels:
  - title: "Overall Quality Score"
    type: "gauge"
    calculation: "Weighted average of all quality checks"
    thresholds:
      - excellent: ">99%"
      - good: "95-99%"
      - needs_attention: "<95%"

  - title: "Completeness"
    type: "stat"
    metrics:
      - null_rate: "Percentage of null values"
      - missing_records: "Expected vs actual row count"

  - title: "Uniqueness"
    type: "stat"
    metrics:
      - duplicate_rate: "Percentage of duplicate records"
      - primary_key_violations: "Count"

  - title: "Freshness"
    type: "stat"
    metrics:
      - data_age: "Time since last update"
      - sla_compliance: "Within freshness SLA"

  - title: "Validity"
    type: "table"
    metrics:
      - schema_compliance: "Matches expected schema"
      - value_range_violations: "Values outside expected ranges"
      - format_violations: "Invalid formats (email, phone, etc.)"

  - title: "Quality Trends"
    type: "graph"
    metrics:
      - quality_score_over_time: "7-day trend"
      - issues_by_category: "Stacked bar chart"
```

---

## Security Considerations

```yaml
security:
  secrets_management:
    - tool: "AWS Secrets Manager / HashiCorp Vault"
    - never_commit: "Database passwords, API keys, credentials"
    - rotation: "Automatic rotation every 90 days"

  data_encryption:
    - at_rest: "AES-256 encryption for S3, databases"
    - in_transit: "TLS 1.2+ for all connections"
    - column_level: "Encrypt PII columns (SSN, credit card)"

  access_control:
    - principle: "Least privilege"
    - authentication: "IAM roles, service accounts"
    - authorization: "RBAC for pipeline resources"

  data_masking:
    - pii_detection: "Automatic detection of PII fields"
    - masking_strategy: "Hash, tokenize, or redact"
    - environments: "Mask in dev/test, plain in prod"

  audit_logging:
    - log_all_access: "Who accessed what data when"
    - retention: "3 years for compliance"
    - tamper_proof: "Write-once storage"
```

---

## Disaster Recovery

```yaml
disaster_recovery:
  backup_strategy:
    - frequency: "Daily full backup, hourly incremental"
    - retention: "30 days daily, 12 months monthly"
    - storage: "Geo-replicated S3 buckets"
    - testing: "Monthly restore tests"

  recovery_time_objective:
    - critical_pipelines: "RTO: 1 hour"
    - standard_pipelines: "RTO: 4 hours"
    - batch_reports: "RTO: 24 hours"

  recovery_point_objective:
    - streaming: "RPO: 5 minutes (last checkpoint)"
    - batch: "RPO: 24 hours (last successful run)"

  failover_procedures:
    - automated: "Health checks trigger automatic failover"
    - manual: "Runbook for manual intervention"
    - testing: "Quarterly disaster recovery drills"
```

---

## Customization Points

After blueprint generation, users can easily customize:

1. **Data sources:** Add extractors in `pipeline/extract/sources/`
2. **Transformations:** Modify business logic in `pipeline/transform/transformations/`
3. **Data quality rules:** Update `pipeline/config/quality_expectations.yaml`
4. **Orchestration schedule:** Edit Airflow DAG schedule in `pipeline/orchestration/airflow/dags/`
5. **Monitoring alerts:** Adjust thresholds in `pipeline/monitoring/alerts/alert_rules.yaml`
6. **Partitioning strategy:** Change in `pipeline/load/partitioning/`

---

## Migration Path

If user starts with blueprint but needs additional features later:

1. **Add CDC:** Run `/skillchain change-data-capture`
2. **Add machine learning:** Run `/skillchain building-ml-models`
3. **Add real-time analytics:** Run `/skillchain real-time-analytics`
4. **Add data governance:** Run `/skillchain data-governance`

All additions will integrate with existing pipeline structure.

---

## Version History

**1.0.0** (2025-12-06)
- Initial data pipeline blueprint
- 6-skill chain with production-ready defaults
- 3-question quick configuration
- Batch, streaming, and hybrid architectures
- Comprehensive data quality framework
- Full observability stack
- Security and disaster recovery

---

## Related Blueprints

- **Dashboard Blueprint:** For visualizing pipeline metrics
- **Database Blueprint:** For schema design and optimization
- **API Blueprint:** For building data APIs on top of warehouse
- **ML Pipeline Blueprint:** For machine learning workflows

---

## Deliverables Specification

This section defines concrete validation checks for blueprint promises, ensuring skills produce what users expect.

### Deliverables

```yaml
deliverables:
  "Pipeline configuration and orchestration":
    primary_skill: transforming-data
    required_files:
      - pipeline/config/pipeline.yaml
      - pipeline/config/connections.yaml
    content_checks:
      - pattern: "source:|destination:|schedule:"
        in: pipeline/config/pipeline.yaml
      - pattern: "host:|port:|database:|credentials:"
        in: pipeline/config/connections.yaml
    maturity_required: [starter, intermediate, advanced]

  "Data extraction layer":
    primary_skill: transforming-data
    required_files:
      - pipeline/extract/sources/
    content_checks:
      - pattern: "def extract|class.*Extractor"
        in: pipeline/extract/sources/
      - pattern: "connect|query|fetch"
        in: pipeline/extract/sources/
    maturity_required: [starter, intermediate, advanced]

  "dbt transformation pipeline":
    primary_skill: transforming-data
    required_files:
      - dbt/dbt_project.yml
      - dbt/models/staging/
      - dbt/models/marts/
    content_checks:
      - pattern: "name:|models:|version:"
        in: dbt/dbt_project.yml
      - pattern: "stg_.*\\.sql"
        in: dbt/models/staging/
      - pattern: "fct_.*\\.sql|dim_.*\\.sql"
        in: dbt/models/marts/
    maturity_required: [starter, intermediate, advanced]

  "SQL transformations":
    primary_skill: transforming-data
    required_files:
      - pipeline/transform/sql/staging_tables.sql
      - pipeline/transform/sql/transformations.sql
    content_checks:
      - pattern: "CREATE TABLE|CREATE OR REPLACE"
        in: pipeline/transform/sql/
      - pattern: "SELECT.*FROM.*WHERE|JOIN"
        in: pipeline/transform/sql/transformations.sql
    maturity_required: [intermediate, advanced]

  "Python data transformations":
    primary_skill: transforming-data
    required_files:
      - pipeline/transform/transformations/cleansing.py
      - pipeline/transform/transformations/business_rules.py
    content_checks:
      - pattern: "def transform|def clean|def enrich"
        in: pipeline/transform/transformations/
      - pattern: "pandas|polars|pyspark"
        in: pipeline/transform/transformations/
    maturity_required: [starter, intermediate, advanced]

  "Data loading strategies":
    primary_skill: transforming-data
    required_files:
      - pipeline/load/sinks/warehouse_loader.py
      - pipeline/load/strategies/incremental.py
    content_checks:
      - pattern: "def load|class.*Loader"
        in: pipeline/load/sinks/
      - pattern: "upsert|merge|insert|copy"
        in: pipeline/load/strategies/
    maturity_required: [starter, intermediate, advanced]

  "Streaming infrastructure (Kafka/Kinesis)":
    primary_skill: streaming-data
    required_files:
      - streaming/kafka/broker-config.yaml
      - streaming/kafka/topic-definitions.yaml
      - streaming/kafka/producer-config.yaml
      - streaming/kafka/consumer-config.yaml
    content_checks:
      - pattern: "broker.id|num.partitions|replication.factor"
        in: streaming/kafka/broker-config.yaml
      - pattern: "topic_name|partitions|retention_ms"
        in: streaming/kafka/topic-definitions.yaml
      - pattern: "bootstrap.servers|acks|idempotence"
        in: streaming/kafka/producer-config.yaml
      - pattern: "group.id|auto.offset.reset|enable.auto.commit"
        in: streaming/kafka/consumer-config.yaml
    maturity_required: [intermediate, advanced]

  "Stream producer implementation":
    primary_skill: streaming-data
    required_files:
      - pipeline/orchestration/streaming/stream_producer.py
    content_checks:
      - pattern: "Producer|produce|send"
        in: pipeline/orchestration/streaming/stream_producer.py
      - pattern: "serialize|schema"
        in: pipeline/orchestration/streaming/
    maturity_required: [intermediate, advanced]

  "Stream consumer with error handling":
    primary_skill: streaming-data
    required_files:
      - pipeline/orchestration/streaming/consumer_groups.py
    content_checks:
      - pattern: "Consumer|subscribe|poll"
        in: pipeline/orchestration/streaming/consumer_groups.py
      - pattern: "dead.letter|retry|error.handling"
        in: pipeline/orchestration/streaming/
    maturity_required: [intermediate, advanced]

  "Avro/Protobuf schemas":
    primary_skill: streaming-data
    required_files:
      - schemas/avro/
    content_checks:
      - pattern: "type.*record|namespace|fields"
        in: schemas/avro/
    maturity_required: [intermediate, advanced]

  "Airflow orchestration DAGs":
    primary_skill: transforming-data
    required_files:
      - pipeline/orchestration/airflow/dags/batch_pipeline.py
    content_checks:
      - pattern: "DAG\\(|@task|PythonOperator"
        in: pipeline/orchestration/airflow/dags/
      - pattern: "schedule_interval|default_args"
        in: pipeline/orchestration/airflow/dags/batch_pipeline.py
    maturity_required: [intermediate, advanced]

  "SQL query optimization":
    primary_skill: optimizing-sql
    required_files:
      - sql/indexes/create-indexes.sql
      - analysis/explain-plan.md
    content_checks:
      - pattern: "CREATE INDEX|CREATE UNIQUE INDEX"
        in: sql/indexes/create-indexes.sql
      - pattern: "EXPLAIN|execution plan|cost"
        in: analysis/explain-plan.md
    maturity_required: [intermediate, advanced]

  "Optimized queries":
    primary_skill: optimizing-sql
    required_files:
      - sql/optimized/queries.sql
      - sql/optimized/before-after.md
    content_checks:
      - pattern: "SELECT|FROM|JOIN|WHERE"
        in: sql/optimized/queries.sql
      - pattern: "before.*after|improvement|optimization"
        in: sql/optimized/before-after.md
    maturity_required: [intermediate, advanced]

  "Data quality validation":
    primary_skill: testing-strategies
    required_files:
      - pipeline/quality/expectations/source_expectations.py
      - pipeline/quality/validators/schema_validator.py
    content_checks:
      - pattern: "great_expectations|ExpectationSuite|expect_"
        in: pipeline/quality/expectations/
      - pattern: "def validate|assert|check"
        in: pipeline/quality/validators/
    maturity_required: [intermediate, advanced]

  "Pipeline unit tests":
    primary_skill: testing-strategies
    required_files:
      - pipeline/tests/unit/test_transformations.py
      - pipeline/tests/unit/test_validators.py
    content_checks:
      - pattern: "def test_|@pytest"
        in: pipeline/tests/unit/
      - pattern: "assert|assertEqual|expect"
        in: pipeline/tests/unit/
    maturity_required: [intermediate, advanced]

  "Pipeline integration tests":
    primary_skill: testing-strategies
    required_files:
      - pipeline/tests/integration/test_end_to_end.py
    content_checks:
      - pattern: "def test_.*pipeline|def test_.*end_to_end"
        in: pipeline/tests/integration/
      - pattern: "extract.*transform.*load"
        in: pipeline/tests/integration/
    maturity_required: [advanced]

  "Prometheus metrics collection":
    primary_skill: implementing-observability
    required_files:
      - pipeline/monitoring/metrics/pipeline_metrics.py
      - pipeline/monitoring/metrics/prometheus_exporter.py
    content_checks:
      - pattern: "Counter|Gauge|Histogram|Summary"
        in: pipeline/monitoring/metrics/pipeline_metrics.py
      - pattern: "prometheus_client|start_http_server"
        in: pipeline/monitoring/metrics/prometheus_exporter.py
    maturity_required: [intermediate, advanced]

  "Structured logging":
    primary_skill: implementing-observability
    required_files:
      - pipeline/monitoring/logging/logger.py
      - pipeline/monitoring/logging/log_config.yaml
    content_checks:
      - pattern: "structlog|logging.config"
        in: pipeline/monitoring/logging/logger.py
      - pattern: "formatters:|handlers:|loggers:"
        in: pipeline/monitoring/logging/log_config.yaml
    maturity_required: [starter, intermediate, advanced]

  "Alert rules":
    primary_skill: implementing-observability
    required_files:
      - pipeline/monitoring/alerts/alert_rules.yaml
    content_checks:
      - pattern: "alert:|expr:|for:|annotations:"
        in: pipeline/monitoring/alerts/alert_rules.yaml
      - pattern: "error_rate|latency|lag"
        in: pipeline/monitoring/alerts/
    maturity_required: [intermediate, advanced]

  "Grafana dashboards":
    primary_skill: implementing-observability
    required_files:
      - pipeline/monitoring/dashboards/grafana/pipeline_dashboard.json
    content_checks:
      - pattern: 'dashboard|panels|targets'
        in: pipeline/monitoring/dashboards/grafana/
      - pattern: 'title.*Pipeline|type.*graph'
        in: pipeline/monitoring/dashboards/grafana/pipeline_dashboard.json
    maturity_required: [intermediate, advanced]

  "Architecture documentation":
    primary_skill: generating-documentation
    required_files:
      - docs/architecture/data_flow_diagram.md
      - docs/architecture/component_diagram.md
    content_checks:
      - pattern: 'mermaid|graph|flowchart'
        in: docs/architecture/data_flow_diagram.md
      - pattern: "source|transform|destination|pipeline"
        in: docs/architecture/
    maturity_required: [starter, intermediate, advanced]

  "Operational runbooks":
    primary_skill: generating-documentation
    required_files:
      - docs/runbooks/deployment.md
      - docs/runbooks/troubleshooting.md
    content_checks:
      - pattern: "##.*Deploy|##.*Step|procedure"
        in: docs/runbooks/deployment.md
      - pattern: "##.*Troubleshoot|##.*Issue|solution"
        in: docs/runbooks/troubleshooting.md
    maturity_required: [intermediate, advanced]

  "Data catalog documentation":
    primary_skill: generating-documentation
    required_files:
      - docs/data_catalog/data_dictionary.md
      - docs/data_catalog/lineage.md
    content_checks:
      - pattern: "table|column|description|type"
        in: docs/data_catalog/data_dictionary.md
      - pattern: "source.*transformation.*destination"
        in: docs/data_catalog/lineage.md
    maturity_required: [intermediate, advanced]

  "Sample data generation":
    primary_skill: transforming-data
    required_files:
      - scripts/generate_sample_data.py
    content_checks:
      - pattern: "def generate|def create"
        in: scripts/generate_sample_data.py
      - pattern: "faker|random|sample"
        in: scripts/generate_sample_data.py
    maturity_required: [starter]

  "Pipeline validation script":
    primary_skill: testing-strategies
    required_files:
      - scripts/validate_pipeline.py
    content_checks:
      - pattern: "def validate|def check"
        in: scripts/validate_pipeline.py
      - pattern: "connection|schema|permissions"
        in: scripts/validate_pipeline.py
    maturity_required: [intermediate, advanced]
```

### Maturity Profiles

```yaml
maturity_profiles:
  starter:
    description: "Simple batch pipeline with basic ETL, local development setup, minimal infrastructure"

    require_additionally:
      - "Pipeline configuration and orchestration"
      - "Data extraction layer"
      - "dbt transformation pipeline"
      - "Python data transformations"
      - "Data loading strategies"
      - "Structured logging"
      - "Architecture documentation"
      - "Sample data generation"

    skip_deliverables:
      - "Streaming infrastructure (Kafka/Kinesis)"
      - "Stream producer implementation"
      - "Stream consumer with error handling"
      - "Avro/Protobuf schemas"
      - "Airflow orchestration DAGs"
      - "SQL query optimization"
      - "Optimized queries"
      - "Data quality validation"
      - "Pipeline unit tests"
      - "Pipeline integration tests"
      - "Prometheus metrics collection"
      - "Alert rules"
      - "Grafana dashboards"
      - "Operational runbooks"
      - "Data catalog documentation"
      - "Pipeline validation script"

    empty_dirs_allowed:
      - pipeline/orchestration/airflow/
      - pipeline/orchestration/streaming/
      - streaming/
      - schemas/
      - pipeline/quality/
      - pipeline/monitoring/dashboards/
      - pipeline/monitoring/alerts/
      - pipeline/tests/integration/
      - sql/optimized/
      - analysis/

    generation_adjustments:
      - Use pandas for small datasets (<10GB)
      - Docker Compose for local dependencies
      - Simple scheduled Python scripts (no Airflow)
      - Basic error handling and retry logic
      - Inline comments for learning
      - README with step-by-step setup
      - Sample CSV/JSON data included

  intermediate:
    description: "Production-ready pipeline with orchestration, monitoring, data quality, and testing"

    require_additionally:
      - "SQL transformations"
      - "Airflow orchestration DAGs"
      - "SQL query optimization"
      - "Optimized queries"
      - "Data quality validation"
      - "Pipeline unit tests"
      - "Prometheus metrics collection"
      - "Alert rules"
      - "Grafana dashboards"
      - "Operational runbooks"
      - "Data catalog documentation"
      - "Pipeline validation script"

    skip_deliverables:
      - "Streaming infrastructure (Kafka/Kinesis)"
      - "Stream producer implementation"
      - "Stream consumer with error handling"
      - "Avro/Protobuf schemas"
      - "Pipeline integration tests"
      - "Sample data generation"

    empty_dirs_allowed:
      - pipeline/orchestration/streaming/
      - streaming/
      - schemas/avro/
      - pipeline/tests/integration/

    generation_adjustments:
      - Use polars for medium datasets (10-100GB)
      - Airflow for orchestration (Docker Compose or Kubernetes)
      - Great Expectations for data quality
      - dbt for SQL transformations
      - Prometheus + Grafana for monitoring
      - Comprehensive error handling and retries
      - Unit tests for transformation logic
      - Documentation for operators and data analysts

  advanced:
    description: "Enterprise-scale with streaming, CDC, distributed processing, comprehensive testing, advanced monitoring"

    require_additionally:
      - "Streaming infrastructure (Kafka/Kinesis)"
      - "Stream producer implementation"
      - "Stream consumer with error handling"
      - "Avro/Protobuf schemas"
      - "Pipeline integration tests"
      - "SQL transformations"
      - "Airflow orchestration DAGs"
      - "SQL query optimization"
      - "Optimized queries"
      - "Data quality validation"
      - "Pipeline unit tests"
      - "Prometheus metrics collection"
      - "Alert rules"
      - "Grafana dashboards"
      - "Operational runbooks"
      - "Data catalog documentation"
      - "Pipeline validation script"

    skip_deliverables:
      - "Sample data generation"

    empty_dirs_allowed: []

    generation_adjustments:
      - Use PySpark for large datasets (100GB+)
      - Kafka or Kinesis for streaming pipelines
      - Airflow on Kubernetes with CeleryExecutor
      - Distributed Flink/Spark Streaming jobs
      - Comprehensive test suite (unit + integration + E2E)
      - Advanced monitoring (SLOs, multi-burn alerts)
      - Schema Registry for Avro/Protobuf
      - CDC with Debezium or Maxwell
      - Terraform for infrastructure provisioning
      - Cost optimization and performance tuning
      - Data lineage tracking
      - Disaster recovery procedures
```

### Validation Process

After all skills complete, the skillchain orchestrator validates deliverables:

1. **File existence checks**: Verify required files exist at specified paths
2. **Content validation**: Use regex patterns to ensure files contain expected code/configuration
3. **Maturity compliance**: Check that maturity-specific deliverables are present
4. **Cross-skill integration**: Validate that skills produced compatible outputs (e.g., dbt models reference correct source tables)
5. **Report generation**: Create validation report showing pass/fail for each deliverable

**Validation command:**
```bash
python scripts/validate_skillchain.py data-pipeline-project/ --blueprint data-pipeline --maturity intermediate
```

**Example validation report:**
```yaml
validation_results:
  passed: 18
  failed: 2
  warnings: 3

  failed_deliverables:
    - deliverable: "Data quality validation"
      reason: "File pipeline/quality/expectations/source_expectations.py missing"
      severity: "error"

    - deliverable: "Grafana dashboards"
      reason: "Pattern '\"dashboard\"' not found in pipeline/monitoring/dashboards/grafana/pipeline_dashboard.json"
      severity: "error"

  warnings:
    - deliverable: "SQL query optimization"
      reason: "File sql/indexes/create-indexes.sql exists but pattern 'CREATE INDEX' not found"
      severity: "warning"
```

### Implementation Notes

**For skill developers:**
- Each skill's `outputs.yaml` defines what files it generates
- Deliverables map blueprint promises to concrete file checks
- Skills should coordinate to avoid duplicate file generation
- Use `primary_skill` to indicate which skill owns the deliverable

**For users:**
- Deliverables specification makes blueprint promises verifiable
- Validation report helps identify missing or incomplete components
- Maturity profiles adjust expectations based on project complexity
- Empty directories are allowed for unused features

**Integration with skillchain orchestrator:**
- After all skills run, orchestrator executes validation
- Validation failures can trigger skill re-runs with adjustments
- Users can customize deliverables for specific needs
- Validation reports aid debugging when pipelines don't work

---

**Blueprint Complete**
